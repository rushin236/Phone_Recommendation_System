{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Recommender Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Imarticus_Learning\\\\12_Projects\\\\Phone_Recommendation_System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Imarticus_Learning\\\\12_Projects\\\\Phone_Recommendation_System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainConfig:\n",
    "    root_dir: Path\n",
    "    model_file: Path\n",
    "    tokenizer_file: Path\n",
    "    transform_data_file: Path\n",
    "    model_evaluation_file: Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainParams:\n",
    "    embedding_dim: int\n",
    "    output_classes: int\n",
    "    epochs: int\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phone_recommender.constants import *\n",
    "from phone_recommender.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH) -> None:\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "    def get_model_build_config(self):\n",
    "        config = self.config.model_training\n",
    "        params = self.params.parameters\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_train_config = ModelTrainConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            model_file=config.model_file,\n",
    "            tokenizer_file=config.tokenizer_file,\n",
    "            transform_data_file=config.transform_data_file,\n",
    "            model_evaluation_file=config.model_evaluation_file,\n",
    "        )\n",
    "\n",
    "        model_train_params = ModelTrainParams(\n",
    "            embedding_dim=params.embedding_dim,\n",
    "            output_classes=params.output_classes,\n",
    "            epochs=params.epochs,\n",
    "            batch_size=params.batch_size,\n",
    "        )\n",
    "\n",
    "        return model_train_config, model_train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-08 00:07:28,115: WARNING: module_wrapper: From d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from phone_recommender.logging import logger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config=ModelTrainConfig, params=ModelTrainParams) -> None:\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "\n",
    "    def get_transformed_data(self):\n",
    "        transform_data_file = self.config.transform_data_file\n",
    "        if not os.path.exists(transform_data_file):\n",
    "            logger.info(\"No transform data file please check if data transform is complete\")\n",
    "        else:\n",
    "            df = pd.read_csv(self.config.transform_data_file)\n",
    "            return df\n",
    "\n",
    "    def build_model(self, df: pd.DataFrame):\n",
    "        text = list(df['text'])\n",
    "        clusters = df['class']\n",
    "\n",
    "        # Initialize the Tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "\n",
    "        # Convert text to sequences of integers\n",
    "        sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "        # Pad sequences to make them of equal length (required for neural networks)\n",
    "        max_sequence_length = max(map(len, sequences))\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "        # Data Sampling\n",
    "        X = pd.DataFrame(padded_sequences)\n",
    "        X.head()\n",
    "\n",
    "        y = to_categorical(clusters)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        embedding_dim = self.params.embedding_dim\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        output_classes = self.params.output_classes\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(output_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "\n",
    "        # Train the model\n",
    "        epochs = self.params.epochs\n",
    "        batch_size = self.params.batch_size\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "        accuracy = model.evaluate(X_test, y_test)[1]\n",
    "        with open(self.config.model_evaluation_file, \"w\") as f:\n",
    "            f.write(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def save_model_tokenizer(self, model: Sequential, tokenizer: Tokenizer):\n",
    "        model.save(self.config.model_file)\n",
    "\n",
    "        # Save the tokenizer using pickle\n",
    "        tokenizer_file = self.config.tokenizer_file\n",
    "\n",
    "        with open(tokenizer_file, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-08 00:07:29,019: INFO: common: yaml file: config\\config.yaml loads successfully]\n",
      "[2023-12-08 00:07:29,021: INFO: common: yaml file: params.yaml loads successfully]\n",
      "[2023-12-08 00:07:29,023: INFO: common: created directory at : artifacts/model_build]\n",
      "[2023-12-08 00:07:29,025: INFO: common: created directory at : artifacts/model_build]\n",
      "[2023-12-08 00:07:29,339: WARNING: module_wrapper: From d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "]\n",
      "[2023-12-08 00:07:29,770: WARNING: module_wrapper: From d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "]\n",
      "Epoch 1/25\n",
      "[2023-12-08 00:07:30,372: WARNING: module_wrapper: From d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "]\n",
      "[2023-12-08 00:07:30,880: WARNING: module_wrapper: From d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "]\n",
      "37/37 [==============================] - 4s 39ms/step - loss: 0.8535 - accuracy: 0.5232 - val_loss: 0.5563 - val_accuracy: 0.6289\n",
      "Epoch 2/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.5460 - accuracy: 0.7191 - val_loss: 0.4533 - val_accuracy: 0.7285\n",
      "Epoch 3/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.4415 - accuracy: 0.7405 - val_loss: 0.3742 - val_accuracy: 0.7869\n",
      "Epoch 4/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.3090 - accuracy: 0.8780 - val_loss: 0.2150 - val_accuracy: 0.9347\n",
      "Epoch 5/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.1795 - accuracy: 0.9442 - val_loss: 0.1997 - val_accuracy: 0.9416\n",
      "Epoch 6/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.1313 - accuracy: 0.9562 - val_loss: 0.1544 - val_accuracy: 0.9519\n",
      "Epoch 7/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.1107 - accuracy: 0.9639 - val_loss: 0.1409 - val_accuracy: 0.9519\n",
      "Epoch 8/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.1050 - accuracy: 0.9596 - val_loss: 0.1901 - val_accuracy: 0.9450\n",
      "Epoch 9/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0906 - accuracy: 0.9734 - val_loss: 0.2462 - val_accuracy: 0.9313\n",
      "Epoch 10/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0806 - accuracy: 0.9768 - val_loss: 0.2706 - val_accuracy: 0.9244\n",
      "Epoch 11/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0796 - accuracy: 0.9794 - val_loss: 0.1457 - val_accuracy: 0.9588\n",
      "Epoch 12/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0656 - accuracy: 0.9794 - val_loss: 0.0920 - val_accuracy: 0.9725\n",
      "Epoch 13/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0518 - accuracy: 0.9863 - val_loss: 0.1394 - val_accuracy: 0.9691\n",
      "Epoch 14/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0690 - accuracy: 0.9811 - val_loss: 0.0977 - val_accuracy: 0.9759\n",
      "Epoch 15/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0254 - accuracy: 0.9931 - val_loss: 0.1541 - val_accuracy: 0.9622\n",
      "Epoch 16/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0247 - accuracy: 0.9914 - val_loss: 0.1236 - val_accuracy: 0.9656\n",
      "Epoch 17/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.1479 - val_accuracy: 0.9691\n",
      "Epoch 18/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0461 - accuracy: 0.9897 - val_loss: 0.1461 - val_accuracy: 0.9691\n",
      "Epoch 19/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.1283 - val_accuracy: 0.9656\n",
      "Epoch 20/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0144 - accuracy: 0.9940 - val_loss: 0.1047 - val_accuracy: 0.9725\n",
      "Epoch 21/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0524 - accuracy: 0.9897 - val_loss: 0.1918 - val_accuracy: 0.9485\n",
      "Epoch 22/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0734 - accuracy: 0.9802 - val_loss: 0.2126 - val_accuracy: 0.9519\n",
      "Epoch 23/25\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 0.0850 - accuracy: 0.9828 - val_loss: 0.1683 - val_accuracy: 0.9553\n",
      "Epoch 24/25\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0614 - accuracy: 0.9837 - val_loss: 0.1599 - val_accuracy: 0.9553\n",
      "Epoch 25/25\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 0.0323 - accuracy: 0.9897 - val_loss: 0.1918 - val_accuracy: 0.9588\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2060 - accuracy: 0.9588\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 54, 50)            29350     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90053 (351.77 KB)\n",
      "Trainable params: 90053 (351.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Imarticus_Learning\\12_Projects\\Phone_Recommendation_System\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_build_config, model_params_config = config.get_model_build_config()\n",
    "    model_trainer = ModelTrainer(model_build_config, model_params_config)\n",
    "    df = model_trainer.get_transformed_data()\n",
    "    model, tokenizer = model_trainer.build_model(df)\n",
    "    model_trainer.save_model_tokenizer(model, tokenizer)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
